{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "from tqdm import tqdm\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from scipy import io\n",
    "import numpy as np\n",
    "import os\n",
    "import sqlalchemy as sqla\n",
    "import yaml\n",
    "import random\n",
    "import string\n",
    "from itertools import permutations\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDict= {\"H\": 'healthy', \"I\": 'inner race', \"O\": 'outer race', \"B\": 'ball', \"C\": \"combination\"}\n",
    "condDict= {\"A\": 'increasing speed', \"B\": 'decreasing speed', \"C\": 'increasing then decreasing speed', \n",
    "           \"D\": 'decreasing then increasing speed'}\n",
    "columnsOrder= ['instance', 'condition', 'label', 'shaft speed', 'vibration velocity']\n",
    "nRows = 2 * 10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_SHARED_CREDENTIALS_FILE'] = r'../.aws/bearingDefectLocator/credentials'\n",
    "os.environ['AWS_CONFIG_FILE'] = r'../.aws/bearingDefectLocator/config'\n",
    "os.environ['DB_CREDENTIALS_FILE'] = r'/home/mattjkenney/projects/.aws/bearingDefectLocator/db.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get database engine URL\n",
    "with open(os.environ['DB_CREDENTIALS_FILE'], 'r') as filehandle:\n",
    "    file = yaml.safe_load(filehandle)\n",
    "\n",
    "engURL = sqla.URL.create(\n",
    "    drivername= \"postgresql+psycopg2\",\n",
    "    username= file.get('master_username'),\n",
    "    password= file.get('master_password'),\n",
    "    port= 5432,\n",
    "    host= file.get('endpoint'),\n",
    "    database=\"bearingvibrations\"     \n",
    "    )\n",
    "print(engURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get keys for s3 objects - stored in pickle file\n",
    "with open('vibs.pk', 'rb') as filehandle:\n",
    "    keyDict = pk.load(filehandle)\n",
    "keyLists = list(keyDict.values())\n",
    "keys = keyLists[0]\n",
    "for k in keyLists[1:]:\n",
    "    keys.extend(k)\n",
    "\n",
    "assert len(keys) == 60 # there should be 60 keys\n",
    "assert [s[-3:] for s in keys].count(\"mat\") == 60 # all keys should end with \"mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for building master dataframe\n",
    "def get_dataframe_from_label(key):\n",
    "\n",
    "    # verify key indentifiers are valid\n",
    "    assert key[-9] in labelDict.keys()\n",
    "    assert key[-7] in condDict.keys()\n",
    "    assert int(key[-5]) in (1,2,3)\n",
    "\n",
    "    # download file from s3\n",
    "    client = boto3.client('s3')\n",
    "    temp_file = 'temp_file.mat'\n",
    "    with open(temp_file, 'wb') as data:\n",
    "        client.download_fileobj('bearingvibrations', key, data)\n",
    "    data.close()\n",
    "\n",
    "    # Load matfile and initilize dataframe\n",
    "    matfile = io.loadmat(temp_file)\n",
    "    dfn = pd.DataFrame()\n",
    "\n",
    "    # Load dataframe with data\n",
    "    dfn['vibration velocity'] = tuple(np.reshape(matfile.get('Channel_1'), (nRows,)))\n",
    "    dfn['shaft speed'] = tuple(np.reshape(matfile.get('Channel_2'), (nRows,)))\n",
    "    dfn['label'] = tuple([labelDict.get(key[-9]) for i in range(nRows)])\n",
    "    dfn['condition'] = tuple([condDict.get(key[-7]) for i in range(nRows)])\n",
    "    dfn['instance'] = tuple([str(key[-5]) for i in range(nRows)])\n",
    "\n",
    "    # Re-organize columns\n",
    "    dfn = dfn.reindex(columns= columnsOrder)\n",
    "\n",
    "    # Final data validations\n",
    "    assert dfn.shape == (2000000, 5) # each dataframe instance should have 2 million rows and 5 columns\n",
    "\n",
    "    # remove temporary file\n",
    "    os.remove(temp_file)\n",
    "\n",
    "    return dfn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of dataframes for all labels\n",
    "dfs = []\n",
    "for i in tqdm(keys):\n",
    "    df = get_dataframe_from_label(i)\n",
    "    assert df.shape == (2000000, 5) # validates shape\n",
    "    assert list(df.columns) == columnsOrder # validates columns and column order\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual inspection of sample dataframe\n",
    "dfs[0].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bts = sum([df.memory_usage(deep=True).sum() for df in dfs])\n",
    "mbs = bts / 1000\n",
    "gbs = mbs / 1000\n",
    "tbs = gbs / 1000\n",
    "print(gbs, 'GB')\n",
    "print(tbs, 'TB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbs * 0.115"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push Data to RDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get DB Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get database engine   \n",
    "engine = sqla.create_engine(engURL)\n",
    "connection = engine.connect()\n",
    "print(connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Table in RDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in tqdm(dfs):\n",
    "    df.to_sql(\"bearing_vibrations\", connection, if_exists='append')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
